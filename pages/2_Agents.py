import os
import re
import json
import time
import pandas as pd
import streamlit as st
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from dotenv import load_dotenv

from langchain_experimental.agents import create_pandas_dataframe_agent
from langchain.agents import AgentType
from langchain.schema import OutputParserException

# LLMs
from langchain_ollama.llms import OllamaLLM
from langchain_google_genai import ChatGoogleGenerativeAI

# Vector store (Chroma) + Embeddings
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# ---------- Setup ----------
st.set_page_config(page_title="OmniCore Hybrid Agents", page_icon="🤖", layout="wide")
load_dotenv()

DATA_PATH = "data/sales_data.csv"
CHROMA_DIR = "./chroma_db"
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
AUDIT_LOG_FILE = "audit_logs.json"

# ---------- Utilities ----------

def safe_extract_output(res) -> str:
    """
    Extract agent output - accept ANY non-empty response from LLM
    """
    if res is None:
        return ""
    if isinstance(res, str):
        return res.strip()
    if isinstance(res, dict):
        # Try common keys in LangChain responses
        for key in ["output", "text", "output_text", "result", "content"]:
            if key in res and res[key]:
                return str(res[key]).strip()
        # Fallback to stringify dict
        return json.dumps(res, ensure_ascii=False)
    # Fallback to str
    return str(res).strip()

def is_empty_or_error_response(output: str) -> bool:
    """
    Only reject if response is truly empty or contains clear error messages
    """
    if not output or len(output.strip()) < 3:
        return True
    
    # Only reject clear error responses
    error_phrases = [
        "i cannot", "i am unable", "sorry, i cannot", 
        "error occurred", "failed to execute", "cannot be completed"
    ]
    
    output_lower = output.lower()
    return any(phrase in output_lower for phrase in error_phrases)

def format_agent_response(raw_output: str, query: str, agent_type: str) -> str:
    """
    Format any valid LLM response consistently without rejecting content
    """
    if not raw_output:
        return raw_output
    
    # Clean basic formatting issues
    formatted = raw_output.strip()
    
    # If it looks like raw pandas output, make it more readable
    if "dtype:" in formatted or "Name:" in formatted:
        lines = formatted.split('\n')
        
        # Try to detect if this is Series output that needs table formatting
        data_lines = [line for line in lines if line.strip() and not line.startswith(('Name:', 'dtype:', 'Length:'))]
        
        if len(data_lines) > 1:
            # Format as table
            table_output = f"""**Analysis Results for:** {query}

| Category | Value |
|----------|-------|
"""
            for line in data_lines:
                if '    ' in line:  # pandas default spacing
                    parts = line.split()
                    if len(parts) >= 2:
                        table_output += f"| {parts[0]} | {parts[-1]} |\n"
            
            table_output += f"\n**Raw Output:**\n```\n{formatted}\n```"
            return table_output
    
    # For other responses, just add minimal formatting
    if not formatted.startswith('**') and not formatted.startswith('#'):
        return f"""**Analysis Results:**

{formatted}

---
*Generated by {agent_type.title()} Agent*"""
    
    return formatted

def stringify_exception(e: Exception, limit: int = 140) -> str:
    msg = f"{type(e).__name__}: {str(e)}"
    return (msg[:limit] + "...") if len(msg) > limit else msg

# ---------- Audit & Tracking ----------
class QueryTracker:
    def __init__(self):
        if 'query_stats' not in st.session_state:
            st.session_state.query_stats = {
                'total_queries': 0,
                'ollama_calls': 0,
                'gemini_calls': 0,
                'fallback_hits': 0,
                'pandas_agent': 0,
                'audit_logs': []
            }
    
    def log_query(self, query: str, complexity: str, execution_path: str, success: bool, response_time: float):
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'query': query,
            'complexity': complexity,
            'execution_path': execution_path,
            'success': success,
            'response_time_ms': response_time * 1000,
        }
        
        st.session_state.query_stats['total_queries'] += 1
        st.session_state.query_stats['audit_logs'].append(log_entry)
        
        # Update path counters
        if execution_path == 'ollama':
            st.session_state.query_stats['ollama_calls'] += 1
        elif execution_path == 'gemini':
            st.session_state.query_stats['gemini_calls'] += 1
        elif execution_path == 'fallback':
            st.session_state.query_stats['fallback_hits'] += 1
        elif execution_path == 'pandas-agent':
            st.session_state.query_stats['pandas_agent'] += 1
            
        # Persist to file (optional, best-effort)
        try:
            with open(AUDIT_LOG_FILE, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
        except Exception:
            pass  # Silent fail for audit logging
    
    def get_stats(self) -> Dict:
        return st.session_state.query_stats

# Global tracker
tracker = QueryTracker()

# ---------- Schema Detection & Management ----------
class SchemaManager:
    def __init__(self):
        if 'detected_schema' not in st.session_state:
            st.session_state.detected_schema = None
    
    def _clean_llm_json(self, text: str) -> str:
        """Clean LLM output to get valid JSON"""
        json_match = re.search(r'\{[\s\S]*\}', text)
        if not json_match:
            return ""
        
        json_str = json_match.group()
        
        # Fix common JSON formatting issues
        json_str = re.sub(r'(\s*)([\w_]+)(\s*):([^"])', r'\1"\2"\3:\4', json_str)
        json_str = re.sub(r"'([^']*)':", r'"\1":', json_str)
        json_str = re.sub(r":\s*'([^']*)'", r':"\1"', json_str)
        json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)
        json_str = re.sub(r':\s*([^"{}\[\],\s][^,}\]]*)', r':"\1"', json_str)
        
        return json_str

    def detect_schema_with_llm(self, df: pd.DataFrame, ollama_llm, gemini_llm) -> Dict:
        """Use LLM to intelligently detect and categorize DataFrame schema"""
        
        llm = gemini_llm if gemini_llm else ollama_llm
        if not llm:
            return self._fallback_schema_detection(df)
        
        sample_data = df.head(3).to_string(index=False, max_cols=10)
        dtypes_info = {col: str(dtype) for col, dtype in df.dtypes.items()}
        
        schema_prompt = f"""Return ONLY a JSON object (no other text) that classifies this DataFrame's schema.
Use ONLY double quotes for all property names and string values.

DataFrame Info:
- Shape: {df.shape[0]} rows × {df.shape[1]} columns
- Column Types: {dtypes_info}

Sample Data:
{sample_data}

Return this exact structure with real column names from the data:
{{
    "numeric_columns": ["list of numeric columns"],
    "categorical_columns": ["list of text/categorical columns"],
    "date_columns": ["list of date columns"],
    "id_columns": ["list of ID columns"],
    "business_metrics": ["list of KPI columns like sales, revenue"],
    "groupby_candidates": ["list of grouping columns"],
    "primary_business_entities": ["main entities"],
    "schema_insights": "brief dataset description"
}}"""

        try:
            if gemini_llm:
                response = gemini_llm.invoke(schema_prompt)
            else:
                response = ollama_llm.invoke(schema_prompt)
            
            response_text = safe_extract_output(response)
            cleaned_json = self._clean_llm_json(response_text)
            
            if cleaned_json:
                try:
                    schema = json.loads(cleaned_json)
                    required_keys = ["numeric_columns", "categorical_columns", "date_columns", 
                                  "business_metrics", "groupby_candidates", "schema_insights"]
                    
                    if all(key in schema for key in required_keys):
                        st.session_state.detected_schema = schema
                        return schema
                    else:
                        st.info("Schema missing required fields → Using fallback")
                except json.JSONDecodeError:
                    st.info("Failed to parse schema JSON → Using fallback")
            
        except Exception as e:
            st.info(f"LLM schema detection failed: {stringify_exception(e)} → Using fallback")
        
        return self._fallback_schema_detection(df)
    
    def _fallback_schema_detection(self, df: pd.DataFrame) -> Dict:
        """Rule-based schema detection as fallback"""
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object', 'category', 'string']).columns.tolist()
        
        # Detect date columns
        date_cols = []
        for col in df.columns:
            lc = col.lower()
            if 'date' in lc or 'time' in lc or 'month' in lc or 'year' in lc:
                try:
                    pd.to_datetime(df[col].head(10), errors='raise')
                    date_cols.append(col)
                except Exception:
                    pass
        
        business_keywords = ['sales', 'sale', 'revenue', 'profit', 'cost', 'price', 'amount', 'value', 'qty', 'quantity', 'margin']
        business_metrics = [col for col in numeric_cols if any(kw in col.lower() for kw in business_keywords)]
        
        groupby_candidates = [col for col in df.columns
                              if (col in categorical_cols or col in date_cols) and df[col].nunique(dropna=True) < max(50, len(df) * 0.5)]
        
        id_keywords = ['id', 'key', 'code', 'number', 'uuid']
        id_cols = [col for col in df.columns if any(kw in col.lower() for kw in id_keywords)]
        
        schema = {
            "numeric_columns": numeric_cols,
            "categorical_columns": categorical_cols,
            "date_columns": date_cols,
            "id_columns": id_cols,
            "business_metrics": business_metrics or numeric_cols[:3],
            "groupby_candidates": groupby_candidates[:10],
            "primary_business_entities": groupby_candidates[:3],
            "schema_insights": f"Dataset with {len(numeric_cols)} numeric and {len(categorical_cols)} categorical columns"
        }
        
        st.session_state.detected_schema = schema
        return schema
    
    def get_schema(self) -> Optional[Dict]:
        return st.session_state.detected_schema

schema_manager = SchemaManager()

# ---------- Cached resources ----------

@st.cache_resource(show_spinner=False)
def get_embeddings():
    try:
        return HuggingFaceEmbeddings(model_name=EMBED_MODEL)
    except Exception as e:
        st.warning(f"Embeddings init failed: {stringify_exception(e)}")
        return None

@st.cache_resource(show_spinner=False)
def get_vector_store():
    if not os.path.exists(CHROMA_DIR):
        return None
    try:
        embs = get_embeddings()
        if not embs:
            return None
        return Chroma(persist_directory=CHROMA_DIR, embedding_function=embs)
    except Exception as e:
        st.warning(f"Chroma init failed: {stringify_exception(e)}")
        return None

@st.cache_resource(show_spinner=False)
def initialize_llms():
    """Initialize both Ollama (local) and Gemini (cloud) once."""
    ollama_llm = None
    gemini_llm = None

    # Ollama
    try:
        ollama_llm = OllamaLLM(
            model="llama3.1",
            temperature=0,
            num_predict=500,
            top_p=0.9,
            repeat_penalty=1.1,
            num_ctx=4096,
            top_k=20,
        )
        # Test connection
        test_response = ollama_llm.invoke("Hello, respond with 'Ollama working'")
        if "working" not in str(test_response).lower():
            st.warning("⚠️ Ollama test response unexpected - may have connection issues")
    except Exception as e:
        st.warning(f"⚠️ Ollama initialization failed: {stringify_exception(e)}")

    # Gemini
    try:
        api_key = os.getenv("GEMINI_API_KEY")
        if api_key:
            gemini_llm = ChatGoogleGenerativeAI(
                model="gemini-2.0-flash-exp",
                google_api_key=api_key,
                temperature=0.1,
                convert_system_message_to_human=True,
            )
            # Test connection
            test_response = gemini_llm.invoke("Hello, respond with 'Gemini working'")
            if "working" not in str(test_response).lower():
                st.warning("⚠️ Gemini test response unexpected - may have connection issues")
        else:
            st.warning("⚠️ GEMINI_API_KEY missing in .env")
    except Exception as e:
        st.warning(f"⚠️ Gemini initialization failed: {stringify_exception(e)}")

    return ollama_llm, gemini_llm

@st.cache_resource(show_spinner=False)
def load_sales_data(path: str):
    """Load CSV to DataFrame with minimal sanity checks."""
    if not os.path.exists(path):
        return None, f"Missing data file at {path}"
    try:
        df = pd.read_csv(path)
        # Optional: ensure datetime if any column named like 'date'
        for col in df.columns:
            if "date" in col.lower():
                with pd.option_context("mode.chained_assignment", None):
                    try:
                        df[col] = pd.to_datetime(df[col], errors="ignore")
                    except Exception:
                        pass
        return df, None
    except Exception as e:
        return None, f"Failed to load data: {stringify_exception(e)}"

# ---------- Agent builders with simplified prompts ----------

def create_ollama_agent(df: pd.DataFrame, ollama_llm: OllamaLLM):
    if not ollama_llm:
        return None
    try:
        return create_pandas_dataframe_agent(
            llm=ollama_llm,
            df=df,
            verbose=True,
            allow_dangerous_code=True,
            handle_parsing_errors=True,
            max_iterations=30,
            agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            return_intermediate_steps=False,
        )
    except Exception as e:
        st.warning(f"Ollama agent creation failed: {stringify_exception(e)}")
        return None

def create_gemini_agent(df: pd.DataFrame, gemini_llm: ChatGoogleGenerativeAI):
    if not gemini_llm:
        return None
    try:
        return create_pandas_dataframe_agent(
            llm=gemini_llm,
            df=df,
            verbose=True,
            allow_dangerous_code=True,
            handle_parsing_errors=True,
            max_iterations=30,
            agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            return_intermediate_steps=False,
        )
    except Exception as e:
        st.warning(f"Gemini agent creation failed: {stringify_exception(e)}")
        return None

# ---------- Query Classification System ----------

def classify_query_complexity(query: str, schema: Dict) -> str:
    """Simplified classification for Simple and Complex only"""
    q = query.lower()
    
    # Check for grouping operations
    grouping_indicators = ['by', 'per', 'for each', 'grouped', 'group', 'across']
    if any(indicator in q for indicator in grouping_indicators):
        time_terms = ['trend', 'over time', 'growth', 'change', 'month', 'year', 'daily', 'weekly']
        analysis_terms = ['analyze', 'analyse', 'analysis', 'insight', 'pattern', 'correlation']
        
        if any(term in q for term in time_terms + analysis_terms):
            return "COMPLEX"
        return "SIMPLE"
    
    # Always route to COMPLEX for advanced analysis
    complex_patterns = [
        'analyze', 'analyse', 'analysis', 'insight', 'pattern', 'trend',
        'correlation', 'relationship', 'predict', 'forecast', 'estimate',
        'describe', 'elaborate', 'explain', 'detail', 'compare', 'contrast',
        'versus', 'vs', 'impact of', 'effect of', 'chart', 'plot', 'graph',
        'visual', 'pie', 'bar', 'histogram', 'scatter'
    ]
    
    if any(pattern in q for pattern in complex_patterns):
        return "COMPLEX"
    
    # Check for time-based analysis
    if any(term in q for term in ['trend', 'over time', 'growth', 'change', 'month', 'year']):
        return "COMPLEX"
    
    # Check for multiple metrics mentioned
    if schema.get('business_metrics'):
        metric_mentions = sum(1 for metric in schema['business_metrics'] 
                            if metric.lower() in q)
        if metric_mentions > 1:
            return "COMPLEX"
    
    return "SIMPLE"

# ---------- Simplified execution strategies ----------

def try_agent_execution(agent, query: str, agent_name: str) -> Optional[str]:
    """
    Try executing query with an agent - accept ANY response that's not empty/error
    """
    if not agent:
        return None
    
    try:
        with st.spinner(f"{agent_name} processing..."):
            res = agent.invoke({"input": query})
            output = safe_extract_output(res)
            
            # Only reject if truly empty or clear error
            if not is_empty_or_error_response(output):
                # Format the response but preserve all content
                formatted_output = format_agent_response(output, query, agent_name)
                return formatted_output
            else:
                st.info(f"{agent_name} returned empty/error response")
                return None
                
    except Exception as e:
        st.info(f"{agent_name} execution failed: {stringify_exception(e)}")
        return None

def execute_simple_strategy(query: str, df: pd.DataFrame, schema: Dict, ollama_llm, gemini_llm) -> Tuple[str, str]:
    """Simple queries - try Ollama first, then Pandas agents, then fallback"""
    start_time = time.time()
    
    # 1. Try Ollama first
    if ollama_llm:
        agent = create_ollama_agent(df, ollama_llm)
        result = try_agent_execution(agent, query, "Ollama")
        if result:
            execution_time = time.time() - start_time
            tracker.log_query(query, "SIMPLE", "ollama", True, execution_time)
            return f"**Ollama Analysis:**\n\n{result}", "ollama"

    # 2. Try Pandas Agent (Gemini preferred)
    if gemini_llm:
        agent = create_gemini_agent(df, gemini_llm)
        result = try_agent_execution(agent, query, "Gemini Pandas")
        if result:
            execution_time = time.time() - start_time
            tracker.log_query(query, "SIMPLE", "pandas-agent", True, execution_time)
            return f"**Pandas Agent Analysis:**\n\n{result}", "pandas-agent"
    
    # 3. Try Pandas Agent with Ollama
    if ollama_llm:
        agent = create_ollama_agent(df, ollama_llm)
        result = try_agent_execution(agent, query, "Ollama Pandas")
        if result:
            execution_time = time.time() - start_time
            tracker.log_query(query, "SIMPLE", "pandas-agent", True, execution_time)
            return f"**Pandas Agent Analysis:**\n\n{result}", "pandas-agent"

    # 4. Final fallback
    execution_time = time.time() - start_time
    tracker.log_query(query, "SIMPLE", "fallback", False, execution_time)
    return enhanced_fallback_analysis(df, query, schema), "fallback"

def execute_complex_strategy(query: str, df: pd.DataFrame, schema: Dict, gemini_llm) -> Tuple[str, str]:
    """Complex queries - try Gemini first, then Pandas agents, then fallback"""
    start_time = time.time()
    
    # Check if this is a chart query
    q = query.lower()
    chart_keywords = ['chart', 'plot', 'graph', 'visuali', 'pie', 'bar', 'histogram', 'scatter', 'line chart']
    is_chart_query = any(keyword in q for keyword in chart_keywords)
    
    # 1. Try Gemini for advanced reasoning
    if gemini_llm:
        agent = create_gemini_agent(df, gemini_llm)
        result = try_agent_execution(agent, query, "Gemini")
        if result:
            # For chart queries, add validation message
            if is_chart_query:
                chart_status = safe_chart_display()
                result += f"\n\n---\n*{chart_status}*"
            
            execution_time = time.time() - start_time
            tracker.log_query(query, "COMPLEX", "gemini", True, execution_time)
            return f"**Gemini Advanced Analysis:**\n\n{result}", "gemini"

    # 2. Fallback to Pandas Agent (try Gemini first, then Ollama)
    if gemini_llm:
        agent = create_gemini_agent(df, gemini_llm)
        result = try_agent_execution(agent, query, "Gemini Pandas")
        if result:
            if is_chart_query:
                chart_status = safe_chart_display()
                result += f"\n\n---\n*{chart_status}*"
            
            execution_time = time.time() - start_time
            tracker.log_query(query, "COMPLEX", "pandas-agent", True, execution_time)
            return f"**Pandas Agent (Complex Fallback):**\n\n{result}", "pandas-agent"
    
    # 3. Final fallback
    execution_time = time.time() - start_time
    tracker.log_query(query, "COMPLEX", "fallback", False, execution_time)
    return enhanced_fallback_analysis(df, query, schema), "fallback"

def execute_hybrid_strategy(query: str, df: pd.DataFrame, schema: Dict, ollama_llm, gemini_llm) -> str:
    """Main router with simplified execution - accept ANY LLM response"""
    complexity = classify_query_complexity(query, schema)
    st.info(f"🎯 Query classified as **{complexity}** | Schema-aware routing")
    
    if complexity == "SIMPLE":
        st.info("🔄 Route: Ollama → Pandas Agent → Enhanced fallback")
        result, path = execute_simple_strategy(query, df, schema, ollama_llm, gemini_llm)
    else:  # COMPLEX
        st.info("💎 Route: Gemini → Pandas Agent → Enhanced fallbacks")
        result, path = execute_complex_strategy(query, df, schema, gemini_llm)
    
    # Add execution path info to result
    path_emoji = {
        "ollama": "🦙", "gemini": "💎", "fallback": "🔧", "pandas-agent": "🐼"
    }
    
    return f"{result}\n\n---\n*Executed via: {path_emoji.get(path, '❓')} {path.upper()}*"

# ---------- Chart Validation Helper ----------

def validate_chart_display() -> bool:
    """Validate that chart was successfully created and displayed"""
    try:
        import matplotlib.pyplot as plt
        if plt.gcf().get_axes():
            return True
        return False
    except Exception:
        return False

def safe_chart_display() -> str:
    """Safely display chart with validation"""
    try:
        import matplotlib.pyplot as plt
        if validate_chart_display():
            st.pyplot(plt.gcf())
            plt.close()
            return "Chart displayed successfully."
        else:
            plt.close()
            return "No chart was generated."
    except Exception as e:
        try:
            plt.close()
        except:
            pass
        return f"Chart display error: {stringify_exception(e)}"

# ---------- Knowledge Base search ----------

def kb_search(query: str) -> str:
    vs = get_vector_store()
    if not vs:
        return "📝 No knowledge base available."
    try:
        docs = vs.similarity_search(query, k=3)
        if not docs:
            return "❓ No relevant information found in knowledge base."
        items = []
        for i, d in enumerate(docs, 1):
            content = d.page_content[:1200]
            items.append(f"**{i}.** {content}")
        return "📖 **Knowledge Base Results:**\n\n" + "\n\n".join(items)
    except Exception as e:
        return f"❌ Knowledge base search failed: {stringify_exception(e)}"

# ---------- Enhanced fallback analysis ----------

def enhanced_fallback_analysis(df: pd.DataFrame, query: str, schema: Dict) -> str:
    """Enhanced fallback using schema information"""
    sample = df.head(2).to_string(index=False, max_cols=8)
    business_metrics = schema.get('business_metrics', [])
    groupby_cols = schema.get('groupby_candidates', [])
    
    return f"""🤔 **Query:** "{query}"

**💡 What I can analyze:**

**🔄 Simple (Ollama + Fallbacks):**
- Group by analysis: {', '.join(groupby_cols[:3]) if groupby_cols else 'categories'}
- Business metrics: {', '.join(business_metrics[:3]) if business_metrics else 'calculations'}
- Rankings and comparisons

**💎 Complex (Gemini + Advanced):**
- Trend analysis and forecasting
- Multi-dimensional correlations
- Business insights and recommendations
- Scenario modeling

**📊 Your Dataset Context:**
- **Shape:** {df.shape[0]} rows × {df.shape[1]} columns  
- **Key Metrics:** {business_metrics[:4] if business_metrics else 'Not detected'}
- **Dimensions:** {groupby_cols[:4] if groupby_cols else 'Not detected'}
- **Schema Insight:** {schema.get('schema_insights', 'Business dataset')}

**Sample:**
{sample}
"""

# ---------- Gemini Response Cleaning ----------
def clean_gemini_response(response_text: str) -> str:
    """Clean up Gemini response by removing excessive markdown formatting"""
    if not response_text or not isinstance(response_text, str):
        return str(response_text) if response_text else ""
    
    try:
        # Remove double asterisks (bold markdown)
        cleaned = response_text.replace('**', '')
        
        # Remove single asterisks at word boundaries (italic markdown)
        import re
        try:
            cleaned = re.sub(r'\*(\w[^*]*\w)\*', r'\1', cleaned)
        except re.error:
            cleaned = cleaned.replace('*', '')
        
        # Remove triple backticks and language specifiers
        try:
            cleaned = re.sub(r'```\w*\n', '', cleaned)
            cleaned = cleaned.replace('```', '')
        except re.error:
            cleaned = cleaned.replace('```', '')
        
        # Clean up excessive newlines
        try:
            cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
        except re.error:
            pass
        
        return cleaned.strip()
        
    except Exception as e:
        st.warning(f"Response cleaning failed: {stringify_exception(e)}")
        return response_text

# ---------- UI ----------

st.title("🤖 OmniCore — Schema-Aware Hybrid AI Agents")
st.caption("Dynamic schema detection • Smart routing • Multi-layer fallbacks • Audit tracking")

# Initialize LLMs once
ollama_llm, gemini_llm = initialize_llms()

# Load and analyze data schema
df, data_err = load_sales_data(DATA_PATH)
schema: Dict = {}

if not data_err and df is not None:
    # Detect schema using LLM
    with st.spinner("🔍 Detecting dataset schema..."):
        schema = schema_manager.detect_schema_with_llm(df, ollama_llm, gemini_llm)

# Sidebar with enhanced status and analytics
with st.sidebar:
    st.subheader("🔧 System Status")

    # Data & Schema status
    if data_err:
        st.error(f"❌ {data_err}")
    else:
        st.success("✅ sales_data.csv ready")
        if df is not None:
            st.write(f"📊 {df.shape[0]} rows × {df.shape[1]} cols")
            
            # Schema insights
            if schema:
                st.success("🧠 Schema detected via LLM")
                with st.expander("📋 Schema Details"):
                    st.write(f"**Business Metrics:** {', '.join(schema.get('business_metrics', [])[:5]) or 'N/A'}")
                    st.write(f"**Grouping Columns:** {', '.join(schema.get('groupby_candidates', [])[:8]) or 'N/A'}")
                    st.write(f"**Date Columns:** {', '.join(schema.get('date_columns', []) or []) or 'N/A'}")
                    st.write(f"**Insights:** {schema.get('schema_insights', 'N/A')}")
            else:
                st.warning("⚠️ Schema detection failed (using rule-based defaults)")

    # LLMs status
    st.markdown("### 🤖 AI Models")
    if ollama_llm:
        st.success("🦙 Ollama Ready")
        st.caption("Local • Fast • Private")
    else:
        st.error("❌ Ollama Unavailable")
        st.caption("Check if Ollama service is running")
    
    if gemini_llm:
        st.success("💎 Gemini Ready")
        st.caption("Cloud • Advanced • Powerful")
    else:
        st.warning("⚠️ Gemini not configured")
        st.caption("Add GEMINI_API_KEY to .env")

    # Knowledge Base
    st.markdown("### 📚 Knowledge Base")
    if get_vector_store():
        st.success("✅ Chroma Ready")
    else:
        st.info("ℹ️ No KB found")

    # Query Analytics
    st.markdown("### 📈 Query Analytics")
    stats = tracker.get_stats()
    if stats['total_queries'] > 0:
        st.metric("Total Queries", stats['total_queries'])
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("🦙 Ollama", stats['ollama_calls'])
            st.metric("💎 Gemini", stats['gemini_calls'])
        with col2:
            st.metric("🔧 Fallbacks", stats['fallback_hits'])
            st.metric("🐼 Pandas Agent", stats['pandas_agent'])
        
        # Success rate
        successful_queries = sum([
            stats['ollama_calls'], stats['gemini_calls'], stats['pandas_agent']
        ])
        success_rate = (successful_queries / stats['total_queries']) * 100
        st.metric("Success Rate", f"{success_rate:.1f}%", help="% queries with meaningful responses")
    else:
        st.info("No queries yet")
    
    # Performance tips
    st.markdown("---")
    st.markdown("### ⚡ Optimization Tips")
    st.write("- Schema auto-detected for smart routing")
    st.write("- Simple: Ollama → Pandas Agent → Enhanced fallback")
    st.write("- Complex: Gemini → Pandas Agent → Enhanced fallbacks")

# Main interface
if data_err:
    st.error(f"❌ {data_err}")
    st.info("""
    **Quick Fix:**
    1. Create a `data/` folder in your project
    2. Place your CSV file as `sales_data.csv`
    3. Or update the `DATA_PATH` variable in the code
    """)
else:
    # Data preview with schema
    with st.expander("📊 Dataset Overview", expanded=False):
        if df is not None:
            col1, col2 = st.columns([1, 1])
            
            with col1:
                st.subheader("📋 Schema Analysis")
                if schema:
                    st.write("**🎯 Business Focus:**")
                    st.write(schema.get('schema_insights', 'Standard dataset'))
                    
                    if schema.get('business_metrics'):
                        st.write("**💰 Key Metrics:**")
                        for metric in schema['business_metrics'][:5]:
                            st.write(f"• {metric}")
                    
                    if schema.get('groupby_candidates'):
                        st.write("**📊 Dimensions:**")
                        for dim in schema['groupby_candidates'][:8]:
                            st.write(f"• {dim}")
                else:
                    st.warning("Schema detection failed")
            
            with col2:
                st.subheader("📄 Sample Data")
                st.dataframe(df.head(8), use_container_width=True)

    # Enhanced routing explanation
    with st.expander("🎯 Smart Routing System"):
        st.markdown("""
**🧠 Schema-Aware Classification:**
- **SIMPLE** → 🦙 Ollama (local) → 🐼 Pandas Agent → Enhanced fallback  
- **COMPLEX** → 💎 Gemini (cloud) → 🐼 Pandas Agent → Enhanced fallback

**🔄 Multi-Layer Fallbacks:**
1. Local LLM processing (Ollama) for simple complexity  
2. Cloud LLM reasoning (Gemini) for complex analysis  
3. Pandas agents as secondary option  
4. Enhanced rule-based analysis as final fallback

**📊 Schema-Driven Optimization:**
- Auto-detects business metrics, dimensions, date columns  
- Routes queries to optimal execution layer  
- Tracks performance for continuous improvement
        """)

    # Dynamic examples based on detected schema
    with st.expander("💡 Schema-Optimized Query Examples"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**🔄 SIMPLE (Ollama+)**") 
            examples = []
            if schema.get('business_metrics') and schema.get('groupby_candidates'):
                metric = schema['business_metrics'][0]
                dimension = schema['groupby_candidates'][0]
                examples = [
                    f"Average {metric} by {dimension}",
                    f"Top 10 {dimension} by {metric}",
                    f"{metric} analysis by category"
                ]
            examples.extend(["Filter data by condition", "Monthly trends"])
            for ex in examples[:6]:
                st.write(f"• {ex}")
        
        with col2:
            st.markdown("**💎 COMPLEX (Gemini)**")
            examples = ["Analyze trends and patterns", "Correlations and insights"]
            if schema.get('primary_business_entities') and len(schema['primary_business_entities']) > 1:
                entities = schema['primary_business_entities'][:2]
                examples.extend([f"Compare {entities[0]} vs {entities[1]}", f"Predict {entities[0]} performance"])
            examples.extend(["Business recommendations", "What-if scenarios", "Create visualizations"])
            for ex in examples[:6]:
                st.write(f"• {ex}")

# Query input with enhanced help
query = st.text_input(
    "🗣️ Ask your question:",
    placeholder="e.g., 'Analyze sales trends by region' or 'Show top 5 products'",
    help="System will auto-detect complexity and route to optimal execution layer"
)

# Mode selection
mode = st.radio(
    "Choose source:",
    options=["Sales Data", "Knowledge Base"],
    horizontal=True,
    index=0,
    help="Select data source for query analysis"
)

# Query execution
if query:
    st.markdown("---")
    with st.spinner("🧠 Processing with schema-aware routing..."):
        try:
            if mode == "Knowledge Base":
                response = kb_search(query)
            else:  # Sales Data
                if df is not None and schema:
                    response = execute_hybrid_strategy(query, df, schema, ollama_llm, gemini_llm)
                else:
                    response = "❌ Sales data not available. Please check if the data file is loaded correctly."

            # Display results
            st.subheader("📝 Analysis Results")
            if isinstance(response, str) and len(response) > 1500:
                st.text_area("Detailed Response:", response, height=500, help="Long response - scroll to see all content")
            else:
                st.markdown(response)

        except Exception as e:
            st.error(f"❌ Error during processing: {stringify_exception(e)}")
            tracker.log_query(query, "ERROR", "error", False, 0)

# Query history and audit (optional)
if st.sidebar.button("📊 Show Query Audit"):
    with st.expander("📈 Query Performance Audit", expanded=True):
        stats = tracker.get_stats()
        if stats['audit_logs']:
            # Recent queries
            st.subheader("🕐 Recent Queries")
            recent_logs = stats['audit_logs'][-10:]  # Last 10 queries
            for log in reversed(recent_logs):
                success_icon = "✅" if log['success'] else "❌"
                path_icon = {
                    "ollama": "🦙", "gemini": "💎",
                    "fallback": "🔧", "pandas-agent": "🐼"
                }.get(log['execution_path'], "❓")
                
                st.write(f"{success_icon} {path_icon} **{log['complexity']}** | {log['response_time_ms']:.0f}ms")
                st.caption(f"Query: {log['query'][:160]}{'...' if len(log['query'])>160 else ''}")
                st.caption(f"Path: {log['execution_path']} | Time: {log['timestamp']}")
                st.divider()
        else:
            st.info("No queries logged yet")

# Footer with system info
st.markdown("---")
col1, col2, col3 = st.columns(3)

with col1:
    st.caption("🤖 **OmniCore v2.0**")
    st.caption("Schema-aware hybrid routing")

with col2:
    if 'df' in locals() and df is not None:
        st.caption(f"📊 **Dataset:** {df.shape[0]}×{df.shape[1]}")
    st.caption("🧠 Dynamic LLM selection")

with col3:
    stats = tracker.get_stats()
    st.caption(f"📈 **Queries:** {stats['total_queries']}")
    if stats['total_queries'] > 0:
        successful_queries = sum([stats['ollama_calls'], stats['gemini_calls'], stats['pandas_agent']])
        success_rate = (successful_queries / stats['total_queries']) * 100
        st.caption(f"✅ **Success:** {success_rate:.1f}%")